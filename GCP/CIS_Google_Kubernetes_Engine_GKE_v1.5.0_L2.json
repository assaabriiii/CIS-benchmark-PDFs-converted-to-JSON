{
    "custom_item_1": {
        "description": "5.5.1 Ensure Container-Optimized OS (cos_containerd) is used for GKE node images",
        "info": "Use Container-Optimized OS (cos_containerd) as a managed, optimized and hardened base OS that limits the host's attack surface.\n\nRationale:\n\nCOS is an operating system image for Compute Engine VMs optimized for running containers. With COS, the containers can be brought up on Google Cloud Platform quickly, efficiently, and securely.\n\nUsing COS as the node image provides the following benefits:\n\nRun containers out of the box: COS instances come pre-installed with the container runtime and cloud-init. With a COS instance, the container can be brought up at the same time as the VM is created, with no on-host setup required.\n\nSmaller attack surface: COS has a smaller footprint, reducing the instance's potential attack surface.\n\nLocked-down by default: COS instances include a locked-down firewall and other security settings by default.\n\nImpact:\n\nIf modifying an existing cluster's Node pool to run COS, the upgrade operation used is long-running and will block other operations on the cluster (including delete) until it has run to completion.\n\nCOS nodes also provide an option with containerd as the main container runtime directly integrated with Kubernetes instead of docker. Thus, on these nodes, Docker cannot view or access containers or images managed by Kubernetes. Applications should not interact with Docker directly. For general troubleshooting or debugging, use crictl instead.",
        "solution": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect the Kubernetes cluster which does not use COS.\n\nUnder the Node pools heading, select the Node Pool that requires alteration.\n\nClick EDIT.\n\nUnder the Image Type heading click CHANGE.\n\nFrom the pop-up menu select Container-optimised OS with containerd (cos_containerd) (default) and click CHANGE\n\nRepeat for all non-compliant Node pools.\n\nUsing Command Line:\nTo set the node image to cos for an existing cluster's Node pool:\n\ngcloud container clusters upgrade--image-type cos_containerd --zone--node-poolDefault Value:\n\nContainer-optimised OS with containerd (cos_containerd) (default) is the default option for a cluster node image.",
        "reference": "800-171|3.4.8,800-53|CM-7(5),800-53|CM-10,800-53r5|CM-7(5),800-53r5|CM-10,CSCv7|5.2,CSCv8|2.5,CSF|DE.CM-3,CSF|PR.IP-1,CSF|PR.PT-3,GDPR|32.1.b,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.5.1,ISO/IEC-27001|A.12.6.2,ITSG-33|CM-7,LEVEL|2A,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,QCSC-v1|3.2,QCSC-v1|8.2.1,SWIFT-CSCv1|2.3,TBA-FIISB|44.2.2,TBA-FIISB|49.2.3",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .nodePools[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\($clusterName), Node Pool Name: \\(.name), Image Type: \\(.config.imageType)\"",
        "regex": "Image Type",
        "expect": "Image Type: COS"
    },
    "custom_item_2": {
        "description": "5.5.7 Ensure Secure Boot for Shielded GKE Nodes is Enabled",
        "info": "Enable Secure Boot for Shielded GKE Nodes to verify the digital signature of node boot components.\n\nRationale:\n\nAn attacker may seek to alter boot components to persist malware or root kits during system initialisation. Secure Boot helps ensure that the system only runs authentic software by verifying the digital signature of all boot components, and halting the boot process if signature verification fails.\n\nImpact:\n\nSecure Boot will not permit the use of third-party unsigned kernel modules.",
        "solution": "Once a Node pool is provisioned, it cannot be updated to enable Secure Boot. New Node pools must be created within the cluster with Secure Boot enabled.\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\n\nFrom the list of clusters, click on the cluster requiring the update and click ADD NODE POOL.\n\nEnsure that the Secure boot checkbox is checked under the Shielded options Heading.\n\nClick SAVE.\n\nWorkloads will need to be migrated from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.\nUsing Command Line:\nTo create a Node pool within the cluster with Secure Boot enabled, run the following command:\n\ngcloud container node-pools create--cluster--zone--shielded-secure-boot\n\nWorkloads will need to be migrated from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.\n\nDefault Value:\n\nBy default, Secure Boot is disabled in GKE clusters. By default, Secure Boot is disabled when Shielded GKE Nodes is enabled.",
        "reference": "800-171|3.11.2,800-171|3.11.3,800-53|RA-5,800-53r5|RA-5,CSCv7|5.3,CSCv8|7.5,CSCv8|7.6,CSF|DE.CM-8,CSF|DE.DP-4,CSF|DE.DP-5,CSF|ID.RA-1,CSF|PR.IP-12,CSF|RS.CO-3,CSF|RS.MI-3,GDPR|32.1.b,GDPR|32.1.d,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.6.1,ITSG-33|RA-5,LEVEL|2A,NESA|M1.2.2,NESA|M5.4.1,NESA|T7.7.1,PCI-DSSv3.2.1|6.1,PCI-DSSv4.0|6.3,PCI-DSSv4.0|6.3.1,QCSC-v1|3.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,SWIFT-CSCv1|2.7",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .nodePools[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\($clusterName), Node Pool Name: \\(.name), Shielded Instance - Enable Secure Boot: \\(.config.shieldedInstanceConfig.enableSecureBoot)\"",
        "regex": "Shielded Instance - Enable Secure Boot",
        "expect": "Shielded Instance - Enable Secure Boot: true"
    },
    "custom_item_3": {
        "description": "5.6.1 Enable VPC Flow Logs and Intranode Visibility",
        "info": "Enable VPC Flow Logs and Intranode Visibility to see pod-level traffic, even for traffic within a worker node.\n\nRationale:\n\nEnabling Intranode Visibility makes intranode pod to pod traffic visible to the networking fabric. With this feature, VPC Flow Logs or other VPC features can be used for intranode traffic.\n\nImpact:\n\nEnabling it on existing cluster causes the cluster master and the cluster nodes to restart, which might cause disruption.",
        "solution": "Enable Intranode Visibility:\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect Kubernetes clusters for which intranode visibility is disabled.\n\nWithin the Details pane, under the Network section, click on the pencil icon named Edit intranode visibility.\n\nCheck the box next to Enable Intranode visibility.\n\nClick SAVE CHANGES.\n\nUsing Command Line:\nTo enable intranode visibility on an existing cluster, run the following command:\n\ngcloud container clusters update--enable-intra-node-visibility\n\nEnable VPC Flow Logs:\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect Kubernetes clusters for which VPC Flow Logs are disabled.\n\nSelect Nodes tab.\n\nSelect Node Pool without VPC Flow Logs enabled.\n\nSelect an Instance Group within the node pool.\n\nSelect an Instance Group Member.\n\nSelect the Subnetwork under Network Interfaces.\n\nClick on EDIT.\n\nSet Flow logs to On.\n\nClick SAVE.\n\nUsing Command Line:\n\nFind the subnetwork name associated with the cluster.\n\ngcloud container clusters describe--region--format json | jq '.subnetwork'\n\nUpdate the subnetwork to enable VPC Flow Logs.\n\ngcloud compute networks subnets update--enable-flow-logs\n\nDefault Value:\n\nBy default, Intranode Visibility is disabled.",
        "reference": "800-171|3.3.1,800-171|3.3.2,800-171|3.3.6,800-53|AU-3,800-53|AU-3(1),800-53|AU-7,800-53|AU-12,800-53r5|AU-3,800-53r5|AU-3(1),800-53r5|AU-7,800-53r5|AU-12,CN-L3|7.1.2.3(a),CN-L3|7.1.2.3(b),CN-L3|7.1.2.3(c),CN-L3|7.1.3.3(a),CN-L3|7.1.3.3(b),CN-L3|8.1.4.3(b),CSCv7|6.3,CSCv8|8.5,CSF|DE.CM-1,CSF|DE.CM-3,CSF|DE.CM-7,CSF|PR.PT-1,CSF|RS.AN-3,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(b),ITSG-33|AU-3,ITSG-33|AU-3(1),ITSG-33|AU-7,ITSG-33|AU-12,LEVEL|2A,NESA|T3.6.2,NIAv2|AM34a,NIAv2|AM34b,NIAv2|AM34c,NIAv2|AM34d,NIAv2|AM34e,NIAv2|AM34f,NIAv2|AM34g,PCI-DSSv3.2.1|10.1,PCI-DSSv3.2.1|10.3,PCI-DSSv3.2.1|10.3.1,PCI-DSSv3.2.1|10.3.2,PCI-DSSv3.2.1|10.3.3,PCI-DSSv3.2.1|10.3.4,PCI-DSSv3.2.1|10.3.5,PCI-DSSv3.2.1|10.3.6,PCI-DSSv4.0|10.2.2,QCSC-v1|3.2,QCSC-v1|6.2,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,QCSC-v1|13.2,SWIFT-CSCv1|6.4",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Enable Intra Node Visibility: \\(.networkConfig.enableIntraNodeVisibility)\"",
        "regex": "Enable Intra Node Visibility",
        "expect": "Enable Intra Node Visibility: true"
    },
    "custom_item_4": {
        "description": "5.6.4 Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled",
        "info": "Disable access to the Kubernetes API from outside the node network if it is not required.\n\nRationale:\n\nIn a private cluster, the master node has two endpoints, a private and public endpoint. The private endpoint is the internal IP address of the master, behind an internal load balancer in the master's VPC network. Nodes communicate with the master using the private endpoint. The public endpoint enables the Kubernetes API to be accessed from outside the master's VPC network.\n\nAlthough Kubernetes API requires an authorized token to perform sensitive actions, a vulnerability could potentially expose the Kubernetes publically with unrestricted access. Additionally, an attacker may be able to identify the current cluster and Kubernetes API version and determine whether it is vulnerable to an attack. Unless required, disabling public endpoint will help prevent such threats, and require the attacker to be on the master's VPC network to perform any attack on the Kubernetes API.\n\nImpact:\n\nTo enable a Private Endpoint, the cluster has to also be configured with private nodes, a private master IP range and IP aliasing enabled.\n\nIf the Private Endpoint flag --enable-private-endpoint is passed to the gcloud CLI, or the external IP address undefined in the Google Cloud Console during cluster creation, then all access from a public IP address is prohibited.",
        "solution": "Once a cluster is created without enabling Private Endpoint only, it cannot be remediated. Rather, the cluster must be recreated.\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n\nClick CREATE CLUSTER, and choose CONFIGURE for the Standard mode cluster.\n\nConfigure the cluster as required then click Networking under CLUSTER in the navigation pane.\n\nUnder IPv4 network access, click the Private cluster radio button.\n\nUncheck the Access control plane using its external IP address checkbox.\n\nIn the Control plane IP range textbox, provide an IP range for the control plane.\n\nConfigure the other settings as required, and click CREATE.\n\nUsing Command Line:\nCreate a cluster with a Private Endpoint enabled and Public Access disabled by including the --enable-private-endpoint flag within the cluster create command:\n\ngcloud container clusters create--enable-private-endpoint\n\nSetting this flag also requires the setting of --enable-private-nodes, --enable-ip-alias and --master-ipv4-cidr=.\n\nDefault Value:\n\nBy default, the Private Endpoint is disabled.",
        "reference": "800-171|3.13.1,800-171|3.13.5,800-171|3.13.6,800-53|CA-9,800-53|SC-7,800-53|SC-7(5),800-53r5|CA-9,800-53r5|SC-7,800-53r5|SC-7(5),CN-L3|7.1.2.2(c),CN-L3|8.1.10.6(j),CSCv7|12,CSCv8|4.4,CSF|DE.CM-1,CSF|ID.AM-3,CSF|PR.AC-5,CSF|PR.DS-5,CSF|PR.PT-4,GDPR|32.1.b,GDPR|32.1.d,GDPR|32.2,HIPAA|164.306(a)(1),ISO/IEC-27001|A.13.1.3,ITSG-33|SC-7,ITSG-33|SC-7(5),LEVEL|2A,NESA|T4.5.4,NIAv2|GS1,NIAv2|GS2a,NIAv2|GS2b,NIAv2|GS7b,NIAv2|NS25,PCI-DSSv3.2.1|1.1,PCI-DSSv3.2.1|1.2,PCI-DSSv3.2.1|1.2.1,PCI-DSSv3.2.1|1.3,PCI-DSSv4.0|1.2.1,PCI-DSSv4.0|1.4.1,QCSC-v1|4.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|6.2,QCSC-v1|8.2.1,SWIFT-CSCv1|2.1,TBA-FIISB|43.1",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Private Cluster Config - Enable Private Endpoint: \\(.privateClusterConfig.enablePrivateEndpoint)\"",
        "regex": "Private Cluster Config - Enable Private Endpoint",
        "expect": "Private Cluster Config - Enable Private Endpoint: true"
    },
    "custom_item_5": {
        "description": "5.10.3 Consider GKE Sandbox for running untrusted workloads",
        "info": "Use GKE Sandbox to restrict untrusted workloads as an additional layer of protection when running in a multi-tenant environment.\n\nRationale:\n\nGKE Sandbox provides an extra layer of security to prevent untrusted code from affecting the host kernel on your cluster nodes.\n\nWhen you enable GKE Sandbox on a Node pool, a sandbox is created for each Pod running on a node in that Node pool. In addition, nodes running sandboxed Pods are prevented from accessing other GCP services or cluster metadata. Each sandbox uses its own userspace kernel.\n\nMulti-tenant clusters and clusters whose containers run untrusted workloads are more exposed to security vulnerabilities than other clusters. Examples include SaaS providers, web-hosting providers, or other organizations that allow their users to upload and run code. A flaw in the container runtime or in the host kernel could allow a process running within a container to 'escape' the container and affect the node's kernel, potentially bringing down the node.\n\nThe potential also exists for a malicious tenant to gain access to and exfiltrate another tenant's data in memory or on disk, by exploiting such a defect.\n\nImpact:\n\nUsing GKE Sandbox requires the node image to be set to Container-Optimized OS with containerd (cos_containerd).\n\nIt is not currently possible to use GKE Sandbox along with the following Kubernetes features:\n\nAccelerators such as GPUs or TPUs\n\nIstio\n\nMonitoring statistics at the level of the Pod or container\n\nHostpath storage\n\nPer-container PID namespace\n\nCPU and memory limits are only applied for Guaranteed Pods and Burstable Pods, and only when CPU and memory limits are specified for all containers running in the Pod\n\nPods using PodSecurityPolicies that specify host namespaces, such as hostNetwork, hostPID, or hostIPC\n\nPods using PodSecurityPolicy settings such as privileged mode\n\nVolumeDevices\n\nPortforward\n\nLinux kernel security modules such as Seccomp, Apparmor, or Selinux Sysctl, NoNewPrivileges, bidirectional MountPropagation, FSGroup, or ProcMount",
        "solution": "Once a node pool is created, GKE Sandbox cannot be enabled, rather a new node pool is required. The default node pool (the first node pool in your cluster, created when the cluster is created) cannot use GKE Sandbox.\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/.\n\nSelect a cluster and click ADD NODE POOL.\n\nConfigure the Node pool with following settings:\n\nFor the node version, select v1.12.6-gke.8 or higher.\n\nFor the node image, select Container-Optimized OS with Containerd (cos_containerd) (default).\n\nUnder Security, select Enable sandbox with gVisor.\n\nConfigure other Node pool settings as required.\n\nClick SAVE.\n\nUsing Command Line:\nTo enable GKE Sandbox on an existing cluster, a new Node pool must be created, which can be done using:\n\n  gcloud container node-pools create--zone--cluster--image-type=cos_containerd --sandbox='type=gvisor'\n\nDefault Value:\n\nBy default, GKE Sandbox is disabled.",
        "reference": "800-171|3.13.1,800-171|3.13.5,800-53|SC-7,800-53r5|SC-7,CN-L3|8.1.10.6(j),CSCv7|18.9,CSCv8|16.8,CSF|DE.CM-1,CSF|PR.AC-5,CSF|PR.DS-5,CSF|PR.PT-4,GDPR|32.1.b,HIPAA|164.306(a)(1),ISO/IEC-27001|A.13.1.3,ITSG-33|SC-7,LEVEL|2M,NESA|T4.5.4,NIAv2|GS1,NIAv2|GS2a,NIAv2|GS2b,PCI-DSSv3.2.1|1.1,PCI-DSSv3.2.1|1.2,PCI-DSSv3.2.1|1.2.1,PCI-DSSv3.2.1|1.3,PCI-DSSv4.0|1.2.1,PCI-DSSv4.0|1.4.1,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|6.2,QCSC-v1|8.2.1,TBA-FIISB|43.1",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Sandbox Config: \\(.nodePools[].config.sandboxConfig.type)\"",
        "regex": "Sandbox Config",
        "expect": "Sandbox Config: gvisor"
    },
    "custom_item_6": {
        "description": "5.10.4 Ensure use of Binary Authorization",
        "info": "Binary Authorization helps to protect supply-chain security by only allowing images with verifiable cryptographically signed metadata into the cluster.\n\nRationale:\n\nBinary Authorization provides software supply-chain security for images that are deployed to GKE from Google Container Registry (GCR) or another container image registry.\n\nBinary Authorization requires images to be signed by trusted authorities during the development process. These signatures are then validated at deployment time. By enforcing validation, tighter control over the container environment can be gained by ensuring only verified images are integrated into the build-and-release process.\n\nImpact:\n\nCare must be taken when defining policy in order to prevent inadvertent denial of container image deployments. Depending on policy, attestations for existing container images running within the cluster may need to be created before those images are redeployed or pulled as part of the pod churn.\n\nTo prevent key system images from being denied deployment, consider the use of global policy evaluation mode, which uses a global policy provided by Google and exempts a list of Google-provided system images from further policy evaluation.",
        "solution": "Using Google Cloud Console\n\nGo to Binary Authorization by visiting: https://console.cloud.google.com/security/binary-authorization.\n\nEnable the Binary Authorization API (if disabled).\n\nCreate an appropriate policy for use with the cluster. See https://cloud.google.com/binary-authorization/docs/policy-yaml-reference for guidance.\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect the cluster for which Binary Authorization is disabled.\n\nUnder the details pane, within the Security section, click on the pencil icon named Edit Binary Authorization.\n\nCheck the box next to Enable Binary Authorization.\n\nChoose whether to Audit, Enforce or both Audit and Enforce the policy and provide a directory for the policy to be used.\n\nClick SAVE CHANGES.\n\nUsing Command Line:\nUpdate the cluster to enable Binary Authorization:\n\ngcloud container cluster update--zone--binauthz-evaluation-mode=See: https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--binauthz-evaluation-mode for more details around the evaluation modes available.\nCreate a Binary Authorization Policy using the Binary Authorization Policy Reference: https://cloud.google.com/binary-authorization/docs/policy-yaml-reference for guidance.\nImport the policy file into Binary Authorization:\n\ngcloud container binauthz policy importDefault Value:\n\nBy default, Binary Authorization is disabled.",
        "reference": "800-171|3.4.1,800-171|3.4.7,800-171|3.4.9,800-53|CM-7(2),800-53|CM-8(3),800-53|CM-10,800-53|CM-11,800-53r5|CM-7(2),800-53r5|CM-8(3),800-53r5|CM-10,800-53r5|CM-11,CN-L3|8.1.10.2(a),CN-L3|8.1.10.2(b),CSCv7|5.2,CSCv8|2.3,CSF|DE.CM-3,CSF|DE.CM-7,CSF|PR.IP-1,CSF|PR.PT-3,GDPR|32.1.b,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.6.2,ITSG-33|CM-7(2),ITSG-33|CM-8(3),LEVEL|2A,NESA|T1.2.1,NESA|T1.2.2,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,QCSC-v1|3.2,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|6.2,QCSC-v1|8.2.1,SWIFT-CSCv1|2.3,SWIFT-CSCv1|5.1",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Binary Authorization - Enabled: \\(.binaryAuthorization.enabled)\"",
        "regex": "Binary Authorization - Enabled",
        "expect": "Binary Authorization - Enabled: true"
    }
}