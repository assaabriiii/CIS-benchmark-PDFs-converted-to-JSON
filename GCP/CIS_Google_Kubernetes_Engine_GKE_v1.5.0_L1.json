{
    "custom_item_1": {
        "description": "5.1.1 Ensure Image Vulnerability Scanning is enabled",
        "info": "Note: GCR is now deprecated, being superseded by Artifact Registry starting 15th May 2024. Runtime Vulnerability scanning is available via GKE Security Posture\n\nScan images stored in Google Container Registry (GCR) or Artifact Registry (AR) for vulnerabilities.\n\nRationale:\n\nVulnerabilities in software packages can be exploited by malicious users to obtain unauthorized access to local cloud resources. GCR Container Analysis API or Artifact Registry Container Scanning API allow images stored in GCR or AR respectively to be scanned for known vulnerabilities.\n\nImpact:\n\nNone.",
        "solution": "For Images Hosted in GCR:\n\nUsing Google Cloud Console\n\nGo to GCR by visiting: https://console.cloud.google.com/gcr\n\nSelect Settings and, under the Vulnerability Scanning heading, click the TURN ON button.\n\nUsing Command Line\n\ngcloud services enable containeranalysis.googleapis.com\n\nFor Images Hosted in AR:\n\nUsing Google Cloud Console\n\nGo to GCR by visiting: https://console.cloud.google.com/artifacts\n\nSelect Settings and, under the Vulnerability Scanning heading, click the ENABLE button.\n\nUsing Command Line\n\ngcloud services enable containerscanning.googleapis.com\n\nDefault Value:\n\nBy default, GCR Container Analysis and AR Container Scanning are disabled.",
        "reference": "800-171|3.11.2,800-171|3.11.3,800-53|RA-5,800-53r5|RA-5,CSCv7|3,CSCv7|3.1,CSCv7|3.2,CSCv8|7.6,CSF|DE.CM-8,CSF|DE.DP-4,CSF|DE.DP-5,CSF|ID.RA-1,CSF|PR.IP-12,CSF|RS.CO-3,CSF|RS.MI-3,GDPR|32.1.b,GDPR|32.1.d,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.6.1,ITSG-33|RA-5,LEVEL|1A,NESA|M1.2.2,NESA|M5.4.1,NESA|T7.7.1,PCI-DSSv3.2.1|6.1,PCI-DSSv4.0|6.3,PCI-DSSv4.0|6.3.1,QCSC-v1|3.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,SWIFT-CSCv1|2.7",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listServices",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.services[] | select(.config.name == \"containerscanning.googleapis.com\") | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Service Name: \\(.config.name), State: \\(.state)\"",
        "regex": "State",
        "expect": "State: ENABLED"
    },
    "custom_item_2": {
        "description": "5.2.1 Ensure GKE clusters are not running using the Compute Engine default service account",
        "info": "Create and use minimally privileged Service accounts to run GKE cluster nodes instead of using the Compute Engine default Service account. Unnecessary permissions could be abused in the case of a node compromise.\n\nRationale:\n\nA GCP service account (as distinct from a Kubernetes ServiceAccount) is an identity that an instance or an application can be used to run GCP API requests. This identity is used to identify virtual machine instances to other Google Cloud Platform services. By default, Kubernetes Engine nodes use the Compute Engine default service account. This account has broad access by default, as defined by access scopes, making it useful to a wide variety of applications on the VM, but it has more permissions than are required to run your Kubernetes Engine cluster.\n\nA minimally privileged service account should be created and used to run the Kubernetes Engine cluster instead of using the Compute Engine default service account, and create separate service accounts for each Kubernetes Workload (See recommendation 5.2.2).\n\nKubernetes Engine requires, at a minimum, the node service account to have the monitoring.viewer, monitoring.metricWriter, and logging.logWriter roles. Additional roles may need to be added for the nodes to pull images from GCR.\n\nImpact:\n\nInstances are automatically granted the https://www.googleapis.com/auth/cloud-platform scope to allow full access to all Google Cloud APIs. This is so that the IAM permissions of the instance are completely determined by the IAM roles of the Service account. Thus if Kubernetes workloads were using cluster access scopes to perform actions using Google APIs, they may no longer be able to, if not permitted by the permissions of the Service account. To remediate, follow recommendation 5.2.2.\n\nThe Service account roles listed here are the minimum required to run the cluster. Additional roles may be required to pull from a private instance of Google Container Registry (GCR).",
        "solution": "Using Google Cloud Console:\nTo create a minimally privileged service account:\n\nGo to Service Accounts by visiting: https://console.cloud.google.com/iam-admin/serviceaccounts.\n\nClick on CREATE SERVICE ACCOUNT.\n\nEnter Service Account Details.\n\nClick CREATE AND CONTINUE.\n\nWithin Service Account permissions add the following roles:\n\nLogs Writer.\n\nMonitoring Metric Writer.\n\n'Monitoring Viewer.\n\nClick CONTINUE.\n\nGrant users access to this service account and create keys as required.\n\nClick DONE.\n\nTo create a Node pool to use the Service account:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nClick on the cluster name within which the Node pool will be launched.\n\nClick on ADD NODE POOL.\n\nWithin the Node Pool details, select the Security subheading, and under 'Identity defaults, select the minimally privileged service account from the Service Account drop-down.\n\nClick 'CREATE to launch the Node pool.\n\nNote: The workloads will need to be migrated to the new Node pool, and the old node pools that use the default service account should be deleted to complete the remediation.\nUsing Command Line:\nTo create a minimally privileged service account:\n\ngcloud iam service-accounts create--display-name 'GKE Node Service Account'\nexport NODE_SA_EMAIL=gcloud iam service-accounts list --format='value(email)' --filter='displayName:GKE Node Service Account'\n\nGrant the following roles to the service account:\n\nexport PROJECT_ID=gcloud config get-value project\ngcloud projects add-iam-policy-binding--member serviceAccount:--role roles/monitoring.metricWriter\ngcloud projects add-iam-policy-binding--member serviceAccount:--role roles/monitoring.viewer\ngcloud projects add-iam-policy-binding--member serviceAccount:--role roles/logging.logWriter\n\nTo create a new Node pool using the Service account, run the following command:\n\ngcloud container node-pools create--service-account=@.iam.gserviceaccount.com--cluster=--zoneNote: The workloads will need to be migrated to the new Node pool, and the old node pools that use the default service account should be deleted to complete the remediation.\n\nDefault Value:\n\nBy default, nodes use the Compute Engine default service account when you create a new cluster.",
        "reference": "800-171|3.5.2,800-53|IA-5,800-53r5|IA-5,CSCv7|4.3,CSCv8|4.7,CSF|PR.AC-1,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(2)(i),HIPAA|164.312(d),ITSG-33|IA-5,LEVEL|1A,NESA|T5.2.3,QCSC-v1|5.2.2,QCSC-v1|13.2",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .nodePools[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\($clusterName), Node Pool Name: \\(.name), Service Account: \\(.config.serviceAccount)\"",
        "regex": "Service Account",
        "expect": "Service Account: (?!default)"
    },
    "custom_item_3": {
        "description": "5.3.1 Ensure Kubernetes Secrets are encrypted using keys managed in Cloud KMS",
        "info": "Encrypt Kubernetes secrets, stored in etcd, at the application-layer using a customer-managed key in Cloud KMS.\n\nRationale:\n\nBy default, GKE encrypts customer content stored at rest, including Secrets. GKE handles and manages this default encryption for you without any additional action on your part.\n\nApplication-layer Secrets Encryption provides an additional layer of security for sensitive data, such as user defined Secrets and Secrets required for the operation of the cluster, such as service account keys, which are all stored in etcd.\n\nUsing this functionality, you can use a key, that you manage in Cloud KMS, to encrypt data at the application layer. This protects against attackers in the event that they manage to gain access to etcd.\n\nImpact:\n\nTo use the Cloud KMS CryptoKey to protect etcd in the cluster, the 'Kubernetes Engine Service Agent' Service account must hold the 'Cloud KMS CryptoKey Encrypter/Decrypter' role.",
        "solution": "To enable Application-layer Secrets Encryption, several configuration items are required. These include:\n\nA key ring\n\nA key\n\nA GKE service account with Cloud KMS CryptoKey Encrypter/Decrypter role\n\nOnce these are created, Application-layer Secrets Encryption can be enabled on an existing or new cluster.\nUsing Google Cloud Console:\nTo create a key\n\nGo to Cloud KMS by visiting https://console.cloud.google.com/security/kms.\n\nSelect CREATE KEY RING.\n\nEnter a Key ring name and the region where the keys will be stored.\n\nClick CREATE.\n\nEnter a Key name and appropriate rotation period within the Create key pane.\n\nClick CREATE.\n\nTo enable on a new cluster\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nClick CREATE CLUSTER, and choose the required cluster mode.\n\nWithin the Security heading, under CLUSTER, check Encrypt secrets at the application layer checkbox.\n\nSelect the kms key as the customer-managed key and, if prompted, grant permissions to the GKE Service account.\n\nClick CREATE.\n\nTo enable on an existing cluster\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect the cluster to be updated.\n\nUnder the Details pane, within the Security heading, click on the pencil named Application-layer secrets encryption.\n\nEnable Encrypt secrets at the application layer and choose a kms key.\n\nClick SAVE CHANGES.\n\nUsing Command Line:\nTo create a key:\nCreate a key ring:\n\ngcloud kms keyrings create--location--projectCreate a key:\n\ngcloud kms keys create--location--keyring--purpose encryption --projectGrant the Kubernetes Engine Service Agent service account the Cloud KMS CryptoKey Encrypter/Decrypter role:\n\ngcloud kms keys add-iam-policy-binding--location--keyring--member serviceAccount:--role roles/cloudkms.cryptoKeyEncrypterDecrypter --projectTo create a new cluster with Application-layer Secrets Encryption:\n\ngcloud container clusters create--cluster-version=latest --zone--database-encryption-key projects//locations//keyRings//cryptoKeys/--projectTo enable on an existing cluster:\n\ngcloud container clusters update--zone--database-encryption-key projects//locations//keyRings//cryptoKeys/--projectDefault Value:\n\nBy default, Application-layer Secrets Encryption is disabled.",
        "reference": "800-171|3.5.2,800-171|3.13.16,800-53|IA-5(1),800-53|SC-28,800-53|SC-28(1),800-53r5|IA-5(1),800-53r5|SC-28,800-53r5|SC-28(1),CN-L3|8.1.4.7(b),CN-L3|8.1.4.8(b),CSCv7|14.8,CSCv8|3.11,CSF|PR.AC-1,CSF|PR.DS-1,GDPR|32.1.a,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(2)(i),HIPAA|164.312(a)(2)(iv),HIPAA|164.312(d),HIPAA|164.312(e)(2)(ii),ITSG-33|IA-5(1),ITSG-33|SC-28,ITSG-33|SC-28a.,ITSG-33|SC-28(1),LEVEL|1A,NESA|T5.2.3,PCI-DSSv3.2.1|3.4,PCI-DSSv4.0|3.3.2,PCI-DSSv4.0|3.5.1,QCSC-v1|5.2.2,QCSC-v1|6.2,QCSC-v1|13.2,SWIFT-CSCv1|4.1,TBA-FIISB|28.1",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Database Encryption - State: \\(.databaseEncryption.state)\"",
        "regex": "Database Encryption - State",
        "expect": "Database Encryption - State: ENCRYPTED"
    },
    "custom_item_4": {
        "description": "5.4.1 Ensure legacy Compute Engine instance metadata APIs are Disabled",
        "info": "Disable the legacy GCE instance metadata APIs for GKE nodes. Under some circumstances, these can be used from within a pod to extract the node's credentials.\n\nRationale:\n\nThe legacy GCE metadata endpoint allows simple HTTP requests to be made returning sensitive information. To prevent the enumeration of metadata endpoints and data exfiltration, the legacy metadata endpoint must be disabled.\n\nWithout requiring a custom HTTP header when accessing the legacy GCE metadata endpoint, a flaw in an application that allows an attacker to trick the code into retrieving the contents of an attacker-specified web URL could provide a simple method for enumeration and potential credential exfiltration. By requiring a custom HTTP header, the attacker needs to exploit an application flaw that allows them to control the URL and also add custom headers in order to carry out this attack successfully.\n\nImpact:\n\nAny workloads using the legacy GCE metadata endpoint will no longer be able to retrieve metadata from the endpoint. Use Workload Identity instead.",
        "reference": "800-171|3.4.2,800-171|3.4.6,800-171|3.4.7,800-53|CM-6,800-53|CM-7,800-53r5|CM-6,800-53r5|CM-7,CSCv7|5.2,CSCv8|16.7,CSF|PR.IP-1,CSF|PR.PT-3,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|CM-6,ITSG-33|CM-7,LEVEL|1A,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,SWIFT-CSCv1|2.3",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .nodePools[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\($clusterName), Node Pool Name: \\(.name), Disable Legacy Endpoints: \\(.config.metadata.\"disable-legacy-endpoints\")\"",
        "regex": "Disable Legacy Endpoints",
        "expect": "Disable Legacy Endpoints: true"
    },
    "custom_item_5": {
        "description": "5.4.2 Ensure the GKE Metadata Server is Enabled",
        "info": "Running the GKE Metadata Server prevents workloads from accessing sensitive instance metadata and facilitates Workload Identity.\n\nRationale:\n\nEvery node stores its metadata on a metadata server. Some of this metadata, such as kubelet credentials and the VM instance identity token, is sensitive and should not be exposed to a Kubernetes workload. Enabling the GKE Metadata server prevents pods (that are not running on the host network) from accessing this metadata and facilitates Workload Identity.\n\nWhen unspecified, the default setting allows running pods to have full access to the node's underlying metadata server.\n\nImpact:\n\nThe GKE Metadata Server must be run when using Workload Identity. Because Workload Identity replaces the need to use Metadata Concealment, the two approaches are incompatible.\n\nWhen the GKE Metadata Server and Workload Identity are enabled, unless the Pod is running on the host network, Pods cannot use the the Compute Engine default service account.\n\nWorkloads may need modification in order for them to use Workload Identity as described within: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity.",
        "solution": "The GKE Metadata Server requires Workload Identity to be enabled on a cluster. Modify the cluster to enable Workload Identity and enable the GKE Metadata Server.\nUsing Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n\nFrom the list of clusters, select the cluster for which Workload Identity is disabled.\n\nUnder the DETAILS pane, navigate down to the Security subsection.\n\nClick on the pencil icon named Edit Workload Identity, click on Enable Workload Identity in the pop-up window, and select a workload pool from the drop-down box. By default, it will be the namespace of the Cloud project containing the cluster, for example:.svc.id.goog.\n\nClick SAVE CHANGES and wait for the cluster to update.\n\nOnce the cluster has updated, select each Node pool within the cluster Details page.\n\nFor each Node pool, select EDIT within the Node pool details page.\n\nWithin the Edit node pool pane, check the Enable GKE Metadata Server checkbox.\n\nClick SAVE.\n\nUsing Command Line\n\ngcloud container clusters update--identity-namespace=.svc.id.goog\n\nNote that existing Node pools are unaffected. New Node pools default to --workload-metadata-from-node=GKE_METADATA_SERVER.\nTo modify an existing Node pool to enable GKE Metadata Server:\n\ngcloud container node-pools update--cluster=--workload-metadata-from-node=GKE_METADATA_SERVER\n\nWorkloads may need modification in order for them to use Workload Identity as described within: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity.\n\nDefault Value:\n\nBy default, running pods to have full access to the node's underlying metadata server.",
        "reference": "800-171|3.4.2,800-171|3.4.6,800-171|3.4.7,800-53|CM-6,800-53|CM-7,800-53r5|CM-6,800-53r5|CM-7,CSCv7|5.2,CSCv8|16.7,CSF|PR.IP-1,CSF|PR.PT-3,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|CM-6,ITSG-33|CM-7,LEVEL|1A,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,SWIFT-CSCv1|2.3",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .nodePools[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\($clusterName), Node Pool Name: \\(.name), Workload Metadata Config - Node Metadata: \\(.config.workloadMetadataConfig.mode)\"",
        "regex": "Workload Metadata Config - Node Metadata",
        "expect": "Workload Metadata Config - Node Metadata: GKE_METADATA"
    },
    "custom_item_6": {
        "description": "5.5.2 Ensure Node Auto-Repair is enabled for GKE nodes",
        "info": "Nodes in a degraded state are an unknown quantity and so may pose a security risk.\n\nRationale:\n\nKubernetes Engine's node auto-repair feature helps you keep the nodes in the cluster in a healthy, running state. When enabled, Kubernetes Engine makes periodic checks on the health state of each node in the cluster. If a node fails consecutive health checks over an extended time period, Kubernetes Engine initiates a repair process for that node.\n\nImpact:\n\nIf multiple nodes require repair, Kubernetes Engine might repair them in parallel. Kubernetes Engine limits number of repairs depending on the size of the cluster (bigger clusters have a higher limit) and the number of broken nodes in the cluster (limit decreases if many nodes are broken).\n\nNode auto-repair is not available on Alpha Clusters.",
        "solution": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\n\nSelect the Kubernetes cluster containing the node pool for which auto-repair is disabled.\n\nSelect the Node pool by clicking on the name of the pool.\n\nNavigate to the Node pool details pane and click EDIT.\n\nUnder the Management heading, check the Enable auto-repair box.\n\nClick SAVE.\n\nRepeat steps 2-6 for every cluster and node pool with auto-upgrade disabled.\n\nUsing Command Line\nTo enable node auto-repair for an existing cluster's Node pool:\n\ngcloud container node-pools update--cluster--zone--enable-autorepair\n\nDefault Value:\n\nNode auto-repair is enabled by default.",
        "reference": "800-171|3.11.2,800-171|3.11.3,800-53|RA-5,800-53r5|RA-5,CSCv7|3.1,CSCv8|7.6,CSF|DE.CM-8,CSF|DE.DP-4,CSF|DE.DP-5,CSF|ID.RA-1,CSF|PR.IP-12,CSF|RS.CO-3,CSF|RS.MI-3,GDPR|32.1.b,GDPR|32.1.d,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.6.1,ITSG-33|RA-5,LEVEL|1A,NESA|M1.2.2,NESA|M5.4.1,NESA|T7.7.1,PCI-DSSv3.2.1|6.1,PCI-DSSv4.0|6.3,PCI-DSSv4.0|6.3.1,QCSC-v1|3.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,SWIFT-CSCv1|2.7",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .nodePools[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\($clusterName), Node Pool Name: \\(.name), Auto Repair: \\(.management.autoRepair)\"",
        "regex": "Auto Repair",
        "expect": "Auto Repair: true"
    },
    "custom_item_7": {
        "description": "5.5.3 Ensure Node Auto-Upgrade is enabled for GKE nodes",
        "info": "Node auto-upgrade keeps nodes at the current Kubernetes and OS security patch level to mitigate known vulnerabilities.\n\nRationale:\n\nNode auto-upgrade helps you keep the nodes in the cluster or node pool up to date with the latest stable patch version of Kubernetes as well as the underlying node operating system. Node auto-upgrade uses the same update mechanism as manual node upgrades.\n\nNode pools with node auto-upgrade enabled are automatically scheduled for upgrades when a new stable Kubernetes version becomes available. When the upgrade is performed, the Node pool is upgraded to match the current cluster master version. From a security perspective, this has the benefit of applying security updates automatically to the Kubernetes Engine when security fixes are released.\n\nImpact:\n\nEnabling node auto-upgrade does not cause the nodes to upgrade immediately. Automatic upgrades occur at regular intervals at the discretion of the Kubernetes Engine team.\n\nTo prevent upgrades occurring during a peak period for the cluster, a maintenance window should be defined. A maintenance window is a four-hour timeframe that can be chosen, during which automatic upgrades should occur. Upgrades can occur on any day of the week, and at any time within the timeframe. To prevent upgrades from occurring during certain dates, a maintenance exclusion should be defined. A maintenance exclusion can span multiple days.",
        "solution": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect the Kubernetes cluster containing the node pool for which auto-upgrade disabled.\n\nSelect the Node pool by clicking on the name of the pool.\n\nNavigate to the Node pool details pane and click EDIT.\n\nUnder the Management heading, check the Enable auto-repair box.\n\nClick SAVE.\n\nRepeat steps 2-6 for every cluster and node pool with auto-upgrade disabled.\n\nUsing Command Line\nTo enable node auto-upgrade for an existing cluster's Node pool, run the following command:\n\ngcloud container node-pools update--cluster--zone--enable-autoupgrade\n\nDefault Value:\n\nNode auto-upgrade is enabled by default.\n\nEven if a cluster has been created with node auto-repair enabled, this only applies to the default Node pool. Subsequent node pools do not have node auto-upgrade enabled by default.",
        "reference": "800-171|3.11.2,800-171|3.11.3,800-171|3.14.1,800-53|RA-5,800-53|SI-2,800-53|SI-2(2),800-53r5|RA-5,800-53r5|SI-2,800-53r5|SI-2(2),CN-L3|8.1.4.4(e),CN-L3|8.1.10.5(a),CN-L3|8.1.10.5(b),CN-L3|8.5.4.1(b),CN-L3|8.5.4.1(d),CN-L3|8.5.4.1(e),CSCv7|2.2,CSCv7|3.4,CSCv7|3.5,CSCv8|7.3,CSF|DE.CM-8,CSF|DE.DP-4,CSF|DE.DP-5,CSF|ID.RA-1,CSF|PR.IP-12,CSF|RS.CO-3,CSF|RS.MI-3,GDPR|32.1.b,GDPR|32.1.d,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.6.1,ITSG-33|RA-5,ITSG-33|SI-2,ITSG-33|SI-2(2),LEVEL|1A,NESA|M1.2.2,NESA|M5.4.1,NESA|T7.6.2,NESA|T7.7.1,NIAv2|PR9,PCI-DSSv3.2.1|6.1,PCI-DSSv3.2.1|6.2,PCI-DSSv4.0|6.3,PCI-DSSv4.0|6.3.1,PCI-DSSv4.0|6.3.3,QCSC-v1|3.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,SWIFT-CSCv1|2.2,SWIFT-CSCv1|2.7",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .nodePools[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\($clusterName), Node Pool Name: \\(.name), Auto Upgrade: \\(.management.autoUpgrade)\"",
        "regex": "Auto Upgrade",
        "expect": "Auto Upgrade: true"
    },
    "custom_item_8": {
        "description": "5.5.5 Ensure Shielded GKE Nodes are Enabled",
        "info": "Shielded GKE Nodes provides verifiable integrity via secure boot, virtual trusted platform module (vTPM)-enabled measured boot, and integrity monitoring.\n\nRationale:\n\nShielded GKE nodes protects clusters against boot- or kernel-level malware or rootkits which persist beyond infected OS.\n\nShielded GKE nodes run firmware which is signed and verified using Google's Certificate Authority, ensuring that the nodes' firmware is unmodified and establishing the root of trust for Secure Boot. GKE node identity is strongly protected via virtual Trusted Platform Module (vTPM) and verified remotely by the master node before the node joins the cluster. Lastly, GKE node integrity (i.e., boot sequence and kernel) is measured and can be monitored and verified remotely.\n\nImpact:\n\nAfter Shielded GKE Nodes is enabled in a cluster, any nodes created in a Node pool without Shielded GKE Nodes enabled, or created outside of any Node pool, aren't able to join the cluster.\n\nShielded GKE Nodes can only be used with Container-Optimized OS (COS), COS with containerd, and Ubuntu node images.",
        "solution": "Note: From version 1.18, clusters will have Shielded GKE nodes enabled by default.\nUsing Google Cloud Console:\nTo update an existing cluster to use Shielded GKE nodes:\n\nNavigate to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect the cluster which for which Shielded GKE Nodes is to be enabled.\n\nWith in the Details pane, under the Security heading, click on the pencil icon named Edit Shields GKE nodes.\n\nCheck the box named Enable Shield GKE nodes.\n\nClick SAVE CHANGES.\n\nUsing Command Line:\nTo migrate an existing cluster, the flag --enable-shielded-nodes needs to be specified in the cluster update command:\n\ngcloud container clusters update--zone--enable-shielded-nodes\n\nDefault Value:\n\nClusters will have Shielded GKE nodes enabled by default, as of version v1.18",
        "reference": "800-171|3.4.2,800-171|3.4.6,800-171|3.4.7,800-53|CM-6,800-53|CM-7,800-53r5|CM-6,800-53r5|CM-7,CSCv7|5.3,CSCv7|18.11,CSCv8|16.7,CSF|PR.IP-1,CSF|PR.PT-3,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|CM-6,ITSG-33|CM-7,LEVEL|1A,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,SWIFT-CSCv1|2.3",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Shielded Nodes - Enabled: \\(.shieldedNodes.enabled)\"",
        "regex": "Shielded Nodes - Enabled",
        "expect": "Shielded Nodes - Enabled: true"
    },
    "custom_item_9": {
        "description": "5.5.6 Ensure Integrity Monitoring for Shielded GKE Nodes is Enabled",
        "info": "Enable Integrity Monitoring for Shielded GKE Nodes to be notified of inconsistencies during the node boot sequence.\n\nRationale:\n\nIntegrity Monitoring provides active alerting for Shielded GKE nodes which allows administrators to respond to integrity failures and prevent compromised nodes from being deployed into the cluster.\n\nImpact:\n\nNone.",
        "solution": "Once a Node pool is provisioned, it cannot be updated to enable Integrity Monitoring. New Node pools must be created within the cluster with Integrity Monitoring enabled.\nUsing Google Cloud Console\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\n\nFrom the list of clusters, click on the cluster requiring the update and click ADD NODE POOL.\n\nEnsure that the 'Integrity monitoring' checkbox is checked under the 'Shielded options' Heading.\n\nClick SAVE.\n\nWorkloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete non-conforming Node pools to complete the remediation\nUsing Command Line\nTo create a Node pool within the cluster with Integrity Monitoring enabled, run the following command:\n\ngcloud container node-pools create--cluster--zone--shielded-integrity-monitoring\n\nWorkloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete non-conforming Node pools to complete the remediation\n\nDefault Value:\n\nIntegrity Monitoring is disabled by default on GKE clusters. Integrity Monitoring is enabled by default for Shielded GKE Nodes; however, if Secure Boot is enabled at creation time, Integrity Monitoring is disabled.",
        "reference": "800-171|3.11.2,800-171|3.11.3,800-53|RA-5,800-53r5|RA-5,CSCv7|5.3,CSCv8|7.5,CSCv8|7.6,CSF|DE.CM-8,CSF|DE.DP-4,CSF|DE.DP-5,CSF|ID.RA-1,CSF|PR.IP-12,CSF|RS.CO-3,CSF|RS.MI-3,GDPR|32.1.b,GDPR|32.1.d,HIPAA|164.306(a)(1),ISO/IEC-27001|A.12.6.1,ITSG-33|RA-5,LEVEL|1A,NESA|M1.2.2,NESA|M5.4.1,NESA|T7.7.1,PCI-DSSv3.2.1|6.1,PCI-DSSv4.0|6.3,PCI-DSSv4.0|6.3.1,QCSC-v1|3.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,SWIFT-CSCv1|2.7",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | .name as $clusterName | .nodePools[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\($clusterName), Node Pool Name: \\(.name), Shielded Instance - Enable Integrity Monitoring: \\(.config.shieldedInstanceConfig.enableIntegrityMonitoring)\"",
        "regex": "Shielded Instance - Enable Integrity Monitoring",
        "expect": "Shielded Instance - Enable Integrity Monitoring: true"
    },
    "custom_item_10": {
        "description": "5.6.2 Ensure use of VPC-native clusters",
        "info": "Create Alias IPs for the node network CIDR range in order to subsequently configure IP-based policies and firewalling for pods. A cluster that uses Alias IPs is called a VPC-native cluster.\n\nRationale:\n\nUsing Alias IPs has several benefits:\n\nPod IPs are reserved within the network ahead of time, which prevents conflict with other compute resources.\n\nThe networking layer can perform anti-spoofing checks to ensure that egress traffic is not sent with arbitrary source IPs.\n\nFirewall controls for Pods can be applied separately from their nodes.\n\nAlias IPs allow Pods to directly access hosted services without using a NAT gateway.\n\nImpact:\n\nYou cannot currently migrate an existing cluster that uses routes for Pod routing to a cluster that uses Alias IPs.\n\nCluster IPs for internal services remain only available from within the cluster. If you want to access a Kubernetes Service from within the VPC, but from outside of the cluster, use an internal load balancer.",
        "solution": "Alias IPs cannot be enabled on an existing cluster. To create a new cluster using Alias IPs, follow the instructions below.\nUsing Google Cloud Console:\nIf using Standard configuration mode:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\n\nClick CREATE CLUSTER, and select Standard configuration mode.\n\nConfigure your cluster as desired , then, click Networking under CLUSTER in the navigation pane.\n\nIn the 'VPC-native' section, leave 'Enable VPC-native (using alias IP)' selected\n\nClick CREATE.\n\nIf using Autopilot configuration mode:\nNote that this is VPC-native only and cannot be disable:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nClick CREATE CLUSTER, and select Autopilot configuration mode.\n\nConfigure your cluster as required\n\nClick CREATE.\n\nUsing Command Line\nTo enable Alias IP on a new cluster, run the following command:\n\ngcloud container clusters create--zone--enable-ip-alias\n\nIf using Autopilot configuration mode:\n\ngcloud container clusters create-auto--zoneDefault Value:\n\nBy default, VPC-native (using alias IP) is enabled when you create a new cluster in the Google Cloud Console, however this is disabled when creating a new cluster using the gcloud CLI, unless the --enable-ip-alias argument is specified.",
        "reference": "800-171|3.13.1,800-171|3.13.5,800-53|CA-9,800-53|SC-7,800-53r5|CA-9,800-53r5|SC-7,CN-L3|8.1.10.6(j),CSCv7|11,CSCv7|14.1,CSCv8|13.4,CSF|DE.CM-1,CSF|ID.AM-3,CSF|PR.AC-5,CSF|PR.DS-5,CSF|PR.PT-4,GDPR|32.1.b,GDPR|32.1.d,GDPR|32.2,HIPAA|164.306(a)(1),ISO/IEC-27001|A.13.1.3,ITSG-33|SC-7,LEVEL|1A,NESA|T4.5.4,NIAv2|GS1,NIAv2|GS2a,NIAv2|GS2b,PCI-DSSv3.2.1|1.1,PCI-DSSv3.2.1|1.2,PCI-DSSv3.2.1|1.2.1,PCI-DSSv3.2.1|1.3,PCI-DSSv4.0|1.2.1,PCI-DSSv4.0|1.4.1,QCSC-v1|4.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|6.2,QCSC-v1|8.2.1,SWIFT-CSCv1|2.1,TBA-FIISB|43.1",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Use IP Aliases: \\(.ipAllocationPolicy.useIpAliases)\"",
        "regex": "Use IP Aliases",
        "expect": "Use IP Aliases: true"
    },
    "custom_item_11": {
        "description": "5.6.3 Ensure Control Plane Authorized Networks is Enabled",
        "info": "Enable Control Plane Authorized Networks to restrict access to the cluster's control plane to only an allowlist of authorized IPs.\n\nRationale:\n\nAuthorized networks are a way of specifying a restricted range of IP addresses that are permitted to access your cluster's control plane. Kubernetes Engine uses both Transport Layer Security (TLS) and authentication to provide secure access to your cluster's control plane from the public internet. This provides you the flexibility to administer your cluster from anywhere; however, you might want to further restrict access to a set of IP addresses that you control. You can set this restriction by specifying an authorized network.\n\nControl Plane Authorized Networks blocks untrusted IP addresses. Google Cloud Platform IPs (such as traffic from Compute Engine VMs) can reach your master through HTTPS provided that they have the necessary Kubernetes credentials.\n\nRestricting access to an authorized network can provide additional security benefits for your container cluster, including:\n\nBetter protection from outsider attacks: Authorized networks provide an additional layer of security by limiting external, non-GCP access to a specific set of addresses you designate, such as those that originate from your premises. This helps protect access to your cluster in the case of a vulnerability in the cluster's authentication or authorization mechanism.\n\nBetter protection from insider attacks: Authorized networks help protect your cluster from accidental leaks of master certificates from your company's premises. Leaked certificates used from outside GCP and outside the authorized IP ranges (for example, from addresses outside your company) are still denied access.\n\nImpact:\n\nWhen implementing Control Plane Authorized Networks, be careful to ensure all desired networks are on the allowlist to prevent inadvertently blocking external access to your cluster's control plane.",
        "solution": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n\nSelect Kubernetes clusters for which Control Plane Authorized Networks is disabled\n\nWithin the Details pane, under the Networking heading, click on the pencil icon named Edit control plane authorised networks.\n\nCheck the box next to Enable control plane authorised networks.\n\nClick SAVE CHANGES.\n\nUsing Command Line:\nTo enable Control Plane Authorized Networks for an existing cluster, run the following command:\n\ngcloud container clusters update--zone--enable-master-authorized-networks\n\nAlong with this, you can list authorized networks using the --master-authorized-networks flag which contains a list of up to 20 external networks that are allowed to connect to your cluster's control plane through HTTPS. You provide these networks as a comma-separated list of addresses in CIDR notation (such as 90.90.100.0/24).\n\nDefault Value:\n\nBy default, Control Plane Authorized Networks is disabled.",
        "reference": "800-171|3.1.1,800-171|3.1.4,800-171|3.1.5,800-171|3.8.1,800-171|3.8.2,800-171|3.8.3,800-53|AC-3,800-53|AC-5,800-53|AC-6,800-53|MP-2,800-53r5|AC-3,800-53r5|AC-5,800-53r5|AC-6,800-53r5|MP-2,CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(g),CN-L3|8.1.4.2(d),CN-L3|8.1.4.2(f),CN-L3|8.1.4.11(b),CN-L3|8.1.10.2(c),CN-L3|8.1.10.6(a),CN-L3|8.5.3.1,CN-L3|8.5.4.1(a),CSCv7|14.6,CSCv8|3.3,CSF|PR.AC-4,CSF|PR.DS-5,CSF|PR.PT-2,CSF|PR.PT-3,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),ISO/IEC-27001|A.6.1.2,ISO/IEC-27001|A.9.4.1,ISO/IEC-27001|A.9.4.5,ITSG-33|AC-3,ITSG-33|AC-5,ITSG-33|AC-6,ITSG-33|MP-2,ITSG-33|MP-2a.,LEVEL|1A,NESA|T1.3.2,NESA|T1.3.3,NESA|T1.4.1,NESA|T4.2.1,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.2,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM3,NIAv2|AM23f,NIAv2|SS13c,NIAv2|SS15c,NIAv2|SS29,PCI-DSSv3.2.1|7.1.2,PCI-DSSv4.0|7.2.1,PCI-DSSv4.0|7.2.2,QCSC-v1|3.2,QCSC-v1|5.2.2,QCSC-v1|6.2,QCSC-v1|13.2,SWIFT-CSCv1|5.1,TBA-FIISB|31.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Master Authorized Networks Config - Enabled: \\(.masterAuthorizedNetworksConfig.enabled)\"",
        "regex": "Master Authorized Networks Config - Enabled",
        "expect": "Master Authorized Networks Config - Enabled: true"
    },
    "custom_item_12": {
        "description": "5.6.5 Ensure clusters are created with Private Nodes",
        "info": "Private Nodes are nodes with no public IP addresses. Disable public IP addresses for cluster nodes, so that they only have private IP addresses.\n\nRationale:\n\nDisabling public IP addresses on cluster nodes restricts access to only internal networks, forcing attackers to obtain local network access before attempting to compromise the underlying Kubernetes hosts.\n\nImpact:\n\nTo enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing enabled.\n\nPrivate Nodes do not have outbound access to the public internet. If you want to provide outbound Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.\n\nTo access Google Cloud APIs and services from private nodes, Private Google Access needs to be set on Kubernetes Engine Cluster Subnets.",
        "solution": "Once a cluster is created without enabling Private Nodes, it cannot be remediated. Rather the cluster must be recreated.\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nClick CREATE CLUSTER.\n\nConfigure the cluster as required then click Networking under CLUSTER in the navigation pane.\n\nUnder IPv4 network access, click the Private cluster radio button.\n\nConfigure the other settings as required, and click CREATE.\n\nUsing Command Line:\nTo create a cluster with Private Nodes enabled, include the --enable-private-nodes flag within the cluster create command:\n\ngcloud container clusters create--enable-private-nodes\n\nSetting this flag also requires the setting of --enable-ip-alias and --master-ipv4-cidr=.\n\nDefault Value:\n\nBy default, Private Nodes are disabled.",
        "reference": "800-171|3.13.1,800-171|3.13.5,800-171|3.13.6,800-53|CA-9,800-53|SC-7,800-53|SC-7(5),800-53r5|CA-9,800-53r5|SC-7,800-53r5|SC-7(5),CN-L3|7.1.2.2(c),CN-L3|8.1.10.6(j),CSCv7|12,CSCv8|4.4,CSF|DE.CM-1,CSF|ID.AM-3,CSF|PR.AC-5,CSF|PR.DS-5,CSF|PR.PT-4,GDPR|32.1.b,GDPR|32.1.d,GDPR|32.2,HIPAA|164.306(a)(1),ISO/IEC-27001|A.13.1.3,ITSG-33|SC-7,ITSG-33|SC-7(5),LEVEL|1A,NESA|T4.5.4,NIAv2|GS1,NIAv2|GS2a,NIAv2|GS2b,NIAv2|GS7b,NIAv2|NS25,PCI-DSSv3.2.1|1.1,PCI-DSSv3.2.1|1.2,PCI-DSSv3.2.1|1.2.1,PCI-DSSv3.2.1|1.3,PCI-DSSv4.0|1.2.1,PCI-DSSv4.0|1.4.1,QCSC-v1|4.2,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|6.2,QCSC-v1|8.2.1,SWIFT-CSCv1|2.1,TBA-FIISB|43.1",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Enable Private Nodes: \\(.privateClusterConfig.enablePrivateNodes)\"",
        "regex": "Enable Private Nodes",
        "expect": "Enable Private Nodes: true"
    },
    "custom_item_13": {
        "description": "5.6.7 Ensure Network Policy is Enabled and set as appropriate",
        "info": "Use Network Policy to restrict pod to pod traffic within a cluster and segregate workloads.\n\nRationale:\n\nBy default, all pod to pod traffic within a cluster is allowed. Network Policy creates a pod-level firewall that can be used to restrict traffic between sources. Pod traffic is restricted by having a Network Policy that selects it (through the use of labels). Once there is any Network Policy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that are not selected by any Network Policy will continue to accept all traffic.\n\nNetwork Policies are managed via the Kubernetes Network Policy API and enforced by a network plugin, simply creating the resource without a compatible network plugin to implement it will have no effect. GKE supports Network Policy enforcement through the use of Calico.\n\nImpact:\n\nNetwork Policy requires the Network Policy add-on. This add-on is included automatically when a cluster with Network Policy is created, but for an existing cluster, needs to be added prior to enabling Network Policy.\n\nEnabling/Disabling Network Policy causes a rolling update of all cluster nodes, similar to performing a cluster upgrade. This operation is long-running and will block other operations on the cluster (including delete) until it has run to completion.\n\nIf Network Policy is used, a cluster must have at least 2 nodes of type n1-standard-1 or higher. The recommended minimum size cluster to run Network Policy enforcement is 3 n1-standard-1 instances.\n\nEnabling Network Policy enforcement consumes additional resources in nodes. Specifically, it increases the memory footprint of the kube-system process by approximately 128MB, and requires approximately 300 millicores of CPU.",
        "solution": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect the cluster for which Network policy is disabled.\n\nUnder the details pane, within the Networking section, click on the pencil icon named Edit network policy.\n\nSet 'Network policy for control plane' to 'Enabled'.\n\nClick SAVE CHANGES.\n\nOnce the cluster has updated, repeat steps 1-3.\n\nSet 'Network Policy for nodes' to 'Enabled'.\n\nClick SAVE CHANGES.\n\nUsing Command Line:\nTo enable Network Policy for an existing cluster, firstly enable the Network Policy add-on:\n\ngcloud container clusters update--zone--update-addons NetworkPolicy=ENABLED\n\nThen, enable Network Policy:\n\ngcloud container clusters update--zone--enable-network-policy\n\nDefault Value:\n\nBy default, Network Policy is disabled.",
        "reference": "800-171|3.1.16,800-171|3.13.15,800-53|AC-18,800-53|SC-23,800-53r5|AC-18,800-53r5|SC-23,CSCv7|9.2,CSCv7|9.4,CSCv8|12.6,CSF|PR.PT-4,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),ITSG-33|AC-18,ITSG-33|SC-23,ITSG-33|SC-23a.,LEVEL|1A,NESA|T4.5.1,QCSC-v1|5.2.1,SWIFT-CSCv1|2.3",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Network Policy - Enabled: \\(.networkPolicy.enabled)\"",
        "regex": "Network Policy - Enabled",
        "expect": "Network Policy - Enabled: true"
    },
    "custom_item_14": {
        "description": "5.7.1 Ensure Logging and Cloud Monitoring is Enabled - loggingService",
        "info": "Send logs and metrics to a remote aggregator to mitigate the risk of local tampering in the event of a breach.\n\nRationale:\n\nExporting logs and metrics to a dedicated, persistent datastore such as Cloud Operations for GKE ensures availability of audit data following a cluster security event, and provides a central location for analysis of log and metric data collated from multiple sources.",
        "solution": "Using Google Cloud Console:\nTo enable Logging:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect the cluster for which Logging is disabled.\n\nUnder the details pane, within the Features section, click on the pencil icon named Edit logging.\n\nCheck the box next to Enable Logging.\n\nIn the drop-down Components box, select the components to be logged.\n\nClick SAVE CHANGES, and wait for the cluster to update.\n\nTo enable Cloud Monitoring:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect the cluster for which Logging is disabled.\n\nUnder the details pane, within the Features section, click on the pencil icon named Edit Cloud Monitoring.\n\nCheck the box next to Enable Cloud Monitoring.\n\nIn the drop-down Components box, select the components to be logged.\n\nClick SAVE CHANGES, and wait for the cluster to update.\n\nUsing Command Line:\nTo enable Logging for an existing cluster, run the following command:\ngcloud container clusters update--zone--logging=See https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--logging for a list of available components for logging.\nTo enable Cloud Monitoring for an existing cluster, run the following command:\ngcloud container clusters update--zone--monitoring=See https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--monitoring for a list of available components for Cloud Monitoring.\n\nDefault Value:\n\nLogging and Cloud Monitoring is enabled by default starting in GKE version 1.14; Legacy Logging and Monitoring support is enabled by default for earlier versions.",
        "reference": "800-171|3.3.1,800-171|3.3.2,800-171|3.3.6,800-53|AU-2,800-53|AU-7,800-53|AU-12,800-53r5|AU-2,800-53r5|AU-7,800-53r5|AU-12,CN-L3|7.1.2.3(c),CN-L3|8.1.4.3(a),CSCv7|6.2,CSCv8|8.2,CSF|DE.CM-1,CSF|DE.CM-3,CSF|DE.CM-7,CSF|PR.PT-1,CSF|RS.AN-3,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(b),ITSG-33|AU-2,ITSG-33|AU-7,ITSG-33|AU-12,LEVEL|1A,NESA|M1.2.2,NESA|M5.5.1,NIAv2|AM7,NIAv2|AM11a,NIAv2|AM11b,NIAv2|AM11c,NIAv2|AM11d,NIAv2|AM11e,NIAv2|SS30,NIAv2|VL8,PCI-DSSv3.2.1|10.1,QCSC-v1|3.2,QCSC-v1|6.2,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,QCSC-v1|13.2,SWIFT-CSCv1|6.4",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Logging Service: \\(.loggingService)\"",
        "regex": "Logging Service",
        "expect": "Logging Service: logging.googleapis.com/kubernetes"
    },
    "custom_item_15": {
        "description": "5.7.1 Ensure Logging and Cloud Monitoring is Enabled - monitoringService",
        "info": "Send logs and metrics to a remote aggregator to mitigate the risk of local tampering in the event of a breach.\n\nRationale:\n\nExporting logs and metrics to a dedicated, persistent datastore such as Cloud Operations for GKE ensures availability of audit data following a cluster security event, and provides a central location for analysis of log and metric data collated from multiple sources.",
        "solution": "Using Google Cloud Console:\nTo enable Logging:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect the cluster for which Logging is disabled.\n\nUnder the details pane, within the Features section, click on the pencil icon named Edit logging.\n\nCheck the box next to Enable Logging.\n\nIn the drop-down Components box, select the components to be logged.\n\nClick SAVE CHANGES, and wait for the cluster to update.\n\nTo enable Cloud Monitoring:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect the cluster for which Logging is disabled.\n\nUnder the details pane, within the Features section, click on the pencil icon named Edit Cloud Monitoring.\n\nCheck the box next to Enable Cloud Monitoring.\n\nIn the drop-down Components box, select the components to be logged.\n\nClick SAVE CHANGES, and wait for the cluster to update.\n\nUsing Command Line:\nTo enable Logging for an existing cluster, run the following command:\ngcloud container clusters update--zone--logging=See https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--logging for a list of available components for logging.\nTo enable Cloud Monitoring for an existing cluster, run the following command:\ngcloud container clusters update--zone--monitoring=See https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--monitoring for a list of available components for Cloud Monitoring.\n\nDefault Value:\n\nLogging and Cloud Monitoring is enabled by default starting in GKE version 1.14; Legacy Logging and Monitoring support is enabled by default for earlier versions.",
        "reference": "800-171|3.3.1,800-171|3.3.2,800-171|3.3.6,800-53|AU-2,800-53|AU-7,800-53|AU-12,800-53r5|AU-2,800-53r5|AU-7,800-53r5|AU-12,CN-L3|7.1.2.3(c),CN-L3|8.1.4.3(a),CSCv7|6.2,CSCv8|8.2,CSF|DE.CM-1,CSF|DE.CM-3,CSF|DE.CM-7,CSF|PR.PT-1,CSF|RS.AN-3,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(b),ITSG-33|AU-2,ITSG-33|AU-7,ITSG-33|AU-12,LEVEL|1A,NESA|M1.2.2,NESA|M5.5.1,NIAv2|AM7,NIAv2|AM11a,NIAv2|AM11b,NIAv2|AM11c,NIAv2|AM11d,NIAv2|AM11e,NIAv2|SS30,NIAv2|VL8,PCI-DSSv3.2.1|10.1,QCSC-v1|3.2,QCSC-v1|6.2,QCSC-v1|8.2.1,QCSC-v1|10.2.1,QCSC-v1|11.2,QCSC-v1|13.2,SWIFT-CSCv1|6.4",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Monitoring Service: \\(.monitoringService)\"",
        "regex": "Monitoring Service",
        "expect": "Monitoring Service: monitoring.googleapis.com/kubernetes"
    },
    "custom_item_16": {
        "description": "5.8.3 Ensure Legacy Authorization (ABAC) is Disabled",
        "info": "Legacy Authorization, also known as Attribute-Based Access Control (ABAC) has been superseded by Role-Based Access Control (RBAC) and is not under active development. RBAC is the recommended way to manage permissions in Kubernetes.\n\nRationale:\n\nIn Kubernetes, RBAC is used to grant permissions to resources at the cluster and namespace level. RBAC allows the definition of roles with rules containing a set of permissions, whilst the legacy authorizer (ABAC) in Kubernetes Engine grants broad, statically defined permissions. As RBAC provides significant security advantages over ABAC, it is recommended option for access control. Where possible, legacy authorization must be disabled for GKE clusters.\n\nImpact:\n\nOnce the cluster has the legacy authorizer disabled, the user must be granted the ability to create authorization roles using RBAC to ensure that the role-based access control permissions take effect.",
        "solution": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\n\nSelect Kubernetes clusters for which Legacy Authorization is enabled.\n\nClick EDIT.\n\nSet 'Legacy Authorization' to 'Disabled'.\n\nClick SAVE.\n\nUsing Command Line:\nTo disable Legacy Authorization for an existing cluster, run the following command:\n\ngcloud container clusters update--zone--no-enable-legacy-authorization\n\nDefault Value:\n\nKubernetes Engine clusters running GKE version 1.8 and later disable the legacy authorization system by default, and thus role-based access control permissions take effect with no special action required.",
        "reference": "800-171|3.1.1,800-171|3.1.5,800-171|3.3.8,800-171|3.3.9,800-53|AC-2,800-53|AC-3,800-53|AC-6,800-53|AC-6(1),800-53|AC-6(7),800-53|AU-9(4),800-53r5|AC-2,800-53r5|AC-3,800-53r5|AC-6,800-53r5|AC-6(1),800-53r5|AC-6(7),800-53r5|AU-9(4),CN-L3|7.1.3.2(b),CN-L3|7.1.3.2(d),CN-L3|7.1.3.2(g),CN-L3|8.1.4.2(d),CN-L3|8.1.4.2(f),CN-L3|8.1.4.3(d),CN-L3|8.1.4.11(b),CN-L3|8.1.10.2(c),CN-L3|8.1.10.6(a),CN-L3|8.5.3.1,CN-L3|8.5.4.1(a),CSCv7|4,CSCv7|16,CSCv8|6.8,CSF|DE.CM-1,CSF|DE.CM-3,CSF|PR.AC-1,CSF|PR.AC-4,CSF|PR.DS-5,CSF|PR.PT-1,CSF|PR.PT-3,GDPR|32.1.b,HIPAA|164.306(a)(1),HIPAA|164.312(a)(1),HIPAA|164.312(b),ISO/IEC-27001|A.9.2.1,ISO/IEC-27001|A.9.2.5,ISO/IEC-27001|A.9.4.1,ISO/IEC-27001|A.9.4.4,ISO/IEC-27001|A.9.4.5,ISO/IEC-27001|A.12.4.2,ITSG-33|AC-2,ITSG-33|AC-3,ITSG-33|AC-6,ITSG-33|AC-6(1),ITSG-33|AU-9(4),ITSG-33|AU-9(4)(a),ITSG-33|AU-9(4)(b),LEVEL|1A,NESA|M1.1.3,NESA|M1.2.2,NESA|M5.2.3,NESA|M5.5.2,NESA|T4.2.1,NESA|T5.1.1,NESA|T5.2.2,NESA|T5.4.1,NESA|T5.4.4,NESA|T5.4.5,NESA|T5.5.4,NESA|T5.6.1,NESA|T7.5.2,NESA|T7.5.3,NIAv2|AM1,NIAv2|AM3,NIAv2|AM23f,NIAv2|AM28,NIAv2|AM31,NIAv2|GS3,NIAv2|GS4,NIAv2|GS8c,NIAv2|NS5j,NIAv2|SM5,NIAv2|SM6,NIAv2|SS13c,NIAv2|SS14e,NIAv2|SS15c,NIAv2|SS29,NIAv2|VL3b,PCI-DSSv3.2.1|7.1.2,PCI-DSSv3.2.1|10.5,PCI-DSSv3.2.1|10.5.2,PCI-DSSv4.0|7.2.1,PCI-DSSv4.0|7.2.2,PCI-DSSv4.0|10.3.2,QCSC-v1|3.2,QCSC-v1|5.2.2,QCSC-v1|6.2,QCSC-v1|8.2.1,QCSC-v1|13.2,QCSC-v1|15.2,SWIFT-CSCv1|5.1,TBA-FIISB|31.1,TBA-FIISB|31.4.2,TBA-FIISB|31.4.3",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Legacy ABAC Enabled: \\(.legacyAbac.enabled)\"",
        "regex": "Legacy ABAC Enabled:",
        "not_expect": "Legacy ABAC Enabled: true"
    },
    "custom_item_17": {
        "description": "5.10.1 Ensure Kubernetes Web UI is Disabled",
        "info": "Note: The Kubernetes web UI (Dashboard) does not have admin access by default in GKE 1.7 and higher. The Kubernetes web UI is disabled by default in GKE 1.10 and higher. In GKE 1.15 and higher, the Kubernetes web UI add-on KubernetesDashboard is no longer supported as a managed add-on.\n\nThe Kubernetes Web UI (Dashboard) has been a historical source of vulnerability and should only be deployed when necessary.\n\nRationale:\n\nYou should disable the Kubernetes Web UI (Dashboard) when running on Kubernetes Engine. The Kubernetes Web UI is backed by a highly privileged Kubernetes Service Account.\n\nThe Google Cloud Console provides all the required functionality of the Kubernetes Web UI and leverages Cloud IAM to restrict user access to sensitive cluster controls and settings.\n\nImpact:\n\nUsers will be required to manage cluster resources using the Google Cloud Console or the command line. These require appropriate permissions. To use the command line, this requires the installation of the command line client, kubectl, on the user's device (this is already included in Cloud Shell) and knowledge of command line operations.",
        "solution": "Using Google Cloud Console:\nCurrently not possible, due to the add-on having been removed. Must use the command line.\nUsing Command Line:\nTo disable the Kubernetes Dashboard on an existing cluster, run the following command:\n\ngcloud container clusters update--zone--update-addons=KubernetesDashboard=DISABLED\n\nDefault Value:\n\nThe Kubernetes web UI (Dashboard) does not have admin access by default in GKE 1.7 and higher. The Kubernetes web UI is disabled by default in GKE 1.10 and higher. In GKE 1.15 and higher, the Kubernetes web UI add-on KubernetesDashboard is no longer supported as a managed add-on.",
        "reference": "800-171|3.4.2,800-171|3.4.6,800-171|3.4.7,800-53|CM-6,800-53|CM-7,800-53r5|CM-6,800-53r5|CM-7,CSCv7|2.2,CSCv7|18.4,CSCv8|4.8,CSF|PR.IP-1,CSF|PR.PT-3,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|CM-6,ITSG-33|CM-7,LEVEL|1A,NIAv2|SS15a,PCI-DSSv3.2.1|2.2.2,SWIFT-CSCv1|2.3",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Kubernetes Dashboard - Disabled: \\(.addonsConfig.kubernetesDashboard.disabled)\"",
        "regex": "Kubernetes Dashboard - Disabled",
        "expect": "Kubernetes Dashboard - Disabled: true"
    },
    "custom_item_18": {
        "description": "5.10.2 Ensure that Alpha clusters are not used for production workloads",
        "info": "Alpha clusters are not covered by an SLA and are not production-ready.\n\nRationale:\n\nAlpha clusters are designed for early adopters to experiment with workloads that take advantage of new features before those features are production-ready. They have all Kubernetes API features enabled, but are not covered by the GKE SLA, do not receive security updates, have node auto-upgrade and node auto-repair disabled, and cannot be upgraded. They are also automatically deleted after 30 days.\n\nImpact:\n\nUsers and workloads will not be able to take advantage of features included within Alpha clusters.",
        "reference": "800-171|3.13.1,800-171|3.13.5,800-53|SC-7,800-53r5|SC-7,CN-L3|8.1.10.6(j),CSCv7|18.9,CSCv8|16.8,CSF|DE.CM-1,CSF|PR.AC-5,CSF|PR.DS-5,CSF|PR.PT-4,GDPR|32.1.b,HIPAA|164.306(a)(1),ISO/IEC-27001|A.13.1.3,ITSG-33|SC-7,LEVEL|1A,NESA|T4.5.4,NIAv2|GS1,NIAv2|GS2a,NIAv2|GS2b,PCI-DSSv3.2.1|1.1,PCI-DSSv3.2.1|1.2,PCI-DSSv3.2.1|1.2.1,PCI-DSSv3.2.1|1.3,PCI-DSSv4.0|1.2.1,PCI-DSSv4.0|1.4.1,QCSC-v1|5.2.1,QCSC-v1|5.2.2,QCSC-v1|6.2,QCSC-v1|8.2.1,TBA-FIISB|43.1",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Enable Kubernetes Alpha: \\(.enableKubernetesAlpha)\"",
        "regex": "Enable Kubernetes Alpha",
        "expect": "Enable Kubernetes Alpha: (null|false)"
    },
    "custom_item_19": {
        "description": "5.10.6 Enable Security Posture",
        "info": "Rationale:\n\nThe security posture dashboard provides insights about your workload security posture at the runtime phase of the software delivery life-cycle.\n\nImpact:\n\nGKE security posture configuration auditing checks your workloads against a set of defined best practices. Each configuration check has its own impact or risk. Learn more about the checks: https://cloud.google.com/kubernetes-engine/docs/concepts/about-configuration-scanning\n\nExample: The host namespace check identifies pods that share host namespaces. Pods that share host namespaces allow Pod processes to communicate with host processes and gather host information, which could lead to a container escape",
        "solution": "Enable security posture via the UI, gCloud or API.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/protect-workload-configuration\n\nDefault Value:\n\nGKE security posture has multiple features. Not all are on by default. Configuration auditing is enabled by default for new standard and autopilot clusters.\n\nsecurityPostureConfig: mode: BASIC",
        "reference": "800-171|3.4.1,800-53|CM-8(3),800-53r5|CM-8(3),CN-L3|8.1.10.2(a),CN-L3|8.1.10.2(b),CSCv7|5.5,CSCv8|2.4,CSF|DE.CM-7,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|CM-8(3),LEVEL|1M,NESA|T1.2.1,NESA|T1.2.2,QCSC-v1|3.2,QCSC-v1|5.2.2,QCSC-v1|5.2.3,QCSC-v1|6.2,QCSC-v1|8.2.1",
        "see_also": "https://workbench.cisecurity.org/benchmarks/13178",
        "request": "listContainerClusters",
        "json_transform": ".projects[] | .projectNumber as $projectNumber | .projectId as $projectId | .value.clusters[] | \"Project Number: \\($projectNumber), Project ID: \\($projectId), Cluster Name: \\(.name), Security Posture Config - Mode: \\(.securityPostureConfig.mode)\"",
        "regex": "Security Posture Config - Mode:",
        "expect": "Security Posture Config - Mode: BASIC"
    }
}